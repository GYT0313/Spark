spark-submit 生产环境
spark-submit --master local[2] \
--class org.apache.spark.examples.streaming.NetworkWordCount \
--name NetworkWorldCount \
/home/hadoop/spark-2.4.0/examples/jars/spark-examples_2.11-2.4.0.jar master 9999



spark-shell 测试环境
spark-shell --master local[2]

import org.apache.spark.streaming.{Seconds, StreamingContext}
val ssc = new StreamingContext(sc, Seconds(1))
val lines = ssc.socketTextStream("master", 9999)
val words = lines.flatMap(_.split(" "))
val wordCounts = words.map(x => (x, 1)).reduceByKey(_ + _)
wordCounts.print()
ssc.start()
ssc.awaitTermination()





##########################
统计单词写入MySQL
CREATE DATABASE gyt_wordcount;
USE gyt_wordcount;
CREATE TABLE wordcount(
id INT NOT NULL PRIMARY KEY AUTO_INCREMENT,
word VARCHAR(50) DEFAULT NULL,
wordcount INT DEFAULT NULL
);

// 插入数据
INSERT INTO wordcount(word, wordcount) VALUES(record._1, record._2);


// 判断是否已经插入过MySQL
SELECT * FROM wordcount WHERE word = record._1;

// 更新操作
UPDATE wordcount SET wordcount = wordcount + record._2 WHERE word = record._1





##########################
统计黑名单 transform

输入： DStream
20190507,zs
20190507,ls
20190507,ww
  --> (zs, (20190507,zs))(ls, (20190507,ls))(ww, (20190507,ww))
 
黑名单: RDD
ww
ls
  --> (zs, ture)(ls, true)


leftJoin:
(zs, ((20190507,zs), true))
(ls, ((20190507,ls), true))
(ww, ((20190507,ww), false))  -- 输出




##########################


Spark Streaming 整合Kafka


1. Receiver
开启zookeeper
开启kafka
kafka-server-start.sh -daemon /home/hadoop/kafka-2.1.1/config/server.properties
创建主题
kafka-topics.sh --zookeeper master:2181 --create --topic kafka_streaming --replication-factor 1 --partitions 1

测试主题

kafka-console-consumer.sh --topic kafka_streaming \
--bootstrap-server slave1:9092 \
--from-beginning

kafka-console-producer.sh --broker-list slave1:9092 \
--topic kafka_streaming




提交到服务器
spark-submit \
--class com.gyt.sparkstream.KafkaDirectWordCount \
--master local[2] \
--name KafkaDirectWordCount \
--packages org.apache.spark:spark-streaming-kafka-0-10_2.11:2.4.0 \
/home/hadoop/lib/sparkstream-1.0-SNAPSHOT.jar \
slave1:9092 kafka_streaming_group3 kafka_streaming





spark-submit \
--class com.gyt.sparkstream.DirectKafkaWordCount \
--master local[2] \
--name DirectKafkaWordCount \
--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:2.4.0 \
/home/hadoop/lib/sparkstream-1.0-SNAPSHOT.jar \
slave1:9092 kafka_streaming_group3 kafka_streaming


package 
